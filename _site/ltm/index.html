<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"June 3, 2025"),r="Latent Thought Models with Variational Bayes Inference-Time Computation",l="We propose Latent Thought Models (LTMs), a novel class of language models that incorporate explicit latent thought vectors to guide autoregressive generation. Through dual-rate optimization and inference-time computation, LTMs achieve superior efficiency and emergent reasoning capabilities at significantly smaller scales than traditional models.";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  date = {${o}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Latent Thought Models | Blogposts</title> <meta name="author" content="Deqian Kong"> <meta name="description" content="We propose Latent Thought Models (LTMs), a novel class of language models that incorporate explicit latent thought vectors to guide autoregressive generation. Through dual-rate optimization and inference-time computation, LTMs achieve superior efficiency and emergent reasoning capabilities at significantly smaller scales than traditional models."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/blogs/assets/css/main.css"> <link rel="canonical" href="/blogs/ltm/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/blogs/assets/js/theme.js"></script> <script src="/blogs/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/blogs/assets/js/distillpub/template.v2.js"></script> <script src="/blogs/assets/js/distillpub/transforms.v2.js"></script> <script src="/blogs/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <d-front-matter> <script async type="text/json">{
      "title": "Latent Thought Models",
      "description": "We propose Latent Thought Models (LTMs), a novel class of language models that incorporate explicit latent thought vectors to guide autoregressive generation. Through dual-rate optimization and inference-time computation, LTMs achieve superior efficiency and emergent reasoning capabilities at significantly smaller scales than traditional models.",
      "published": "June 3, 2025",
      "authors": [
        {
          "author": "Deqian Kong",
          "authorURL": "https://sites.google.com/view/deqiankong/",
          "affiliations": [
            {
              "name": "UCLA",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blogs/">Blogposts</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Latent Thought Models <br><span style="font-size: 0.6em; font-weight: bold; display: block; margin-top: 0.2em;">with Variational Bayes Inference-Time Computation</span> </h1> <p>We propose Latent Thought Models (LTMs), a novel class of language models that incorporate explicit latent thought vectors to guide autoregressive generation. Through dual-rate optimization and inference-time computation, LTMs achieve superior efficiency and emergent reasoning capabilities at significantly smaller scales than traditional models.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#the-problem-with-current-ai">The Problem with Current AI</a></div> <div><a href="#images-and-figures">Images and Figures</a></div> <ul> <li><a href="#interactive-figures">Interactive Figures</a></li> </ul> <div><a href="#citations">Citations</a></div> <div><a href="#footnotes">Footnotes</a></div> <div><a href="#code-blocks">Code Blocks</a></div> <div><a href="#diagrams">Diagrams</a></div> <div><a href="#tweets">Tweets</a></div> <div><a href="#layouts">Layouts</a></div> <div><a href="#other-typography">Other Typography?</a></div> </nav> </d-contents> <h2 id="the-problem-with-current-ai">The Problem with Current AI</h2> <p>Think about how you write. Before putting pen to paper (or fingers to keyboard), your mind forms an abstract understanding of what you want to express. You might think about the main themes, the emotional tone, or the logical structure. Only then do you translate these abstract thoughts into concrete words.</p> <p>Current language models like GPT work differently. They generate text token by token, word by word, without any higher-level planning or abstract representation. It’s like speaking without thinking—impressive, but ultimately limited.</p> <div class="key-insight"> <strong>Key Insight:</strong> Traditional language models lack explicit mechanisms for abstract reasoning and planning, limiting their ability to perform complex cognitive tasks efficiently. </div> <h2 id="our-solution-latent-thought-models">Our Solution: Latent Thought Models</h2> <p>We propose <strong>Latent Thought Models (LTMs)</strong>—a new class of language models that explicitly learn to form abstract “thoughts” before generating text. These thoughts are represented as latent vectors that capture the essence of what the model wants to express, before translating it into actual words.</p> <h3 id="core-architecture">Core Architecture</h3> <p>Imagine an AI that works more like a human writer:</p> <ol> <li> <strong>Think first</strong>: Form abstract thoughts about the content, themes, and structure</li> <li> <strong>Write second</strong>: Use these thoughts to guide the actual text generation</li> </ol> <p>Our LTMs do exactly this through a two-stage process:</p> \[\text{Abstract Thoughts } (\mathbf{z}) \rightarrow \text{Concrete Words } (\mathbf{x})\] <p>The latent thought vectors $\mathbf{z}$ serve as a compressed, abstract representation that guides the generation of each token in the sequence.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blogs/assets/img/ltm/ltm_architecture-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blogs/assets/img/ltm/ltm_architecture-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blogs/assets/img/ltm/ltm_architecture-1400.webp"></source> <img src="/blogs/assets/img/ltm/ltm_architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>LTM Architecture:</b> Latent thought vectors $\mathbf{z}$ are sampled from a prior distribution and guide autoregressive generation through cross-attention at each layer. </div> <h4 id="layered-thought-vectors">Layered Thought Vectors</h4> <p>Instead of having just one set of thoughts, our models use <strong>layered thought vectors</strong>—different abstract representations for different layers of the neural network. This creates a hierarchy of abstraction:</p> <ul> <li> <strong>Lower layers</strong>: Basic linguistic patterns and syntax</li> <li> <strong>Middle layers</strong>: Semantic relationships and concepts</li> <li> <strong>Higher layers</strong>: High-level themes and narrative structure</li> </ul> <p>We assume $\mathbf{z} = (\mathbf{z}_1, …, \mathbf{z}_L)$, where $\mathbf{z}_l$ consists of thought vectors cross-attending to layer $l$ of the Transformer decoder.</p> <h4 id="thought-guided-generation">Thought-Guided Generation</h4> <table> <tbody> <tr> <td>The key component is a thought-conditioned autoregressive generator $p_{\beta}(\mathbf{x}</td> <td>\mathbf{z})$:</td> </tr> </tbody> </table> <div class="equation-highlight"> $$p_{\beta}(\mathbf{x}|\mathbf{z}) = \prod_{n=1}^N p_{\beta}(x^{(n)}|\mathbf{z}, \mathbf{x}^{(&lt;n)})$$ </div> <p>Unlike standard autoregressive models that only condition on previous tokens, our model incorporates the thought vectors $\mathbf{z}$ at each generation step through cross-attention.</p> <h3 id="dual-rate-learning-algorithm">Dual-Rate Learning Algorithm</h3> <p>Our training process mirrors human learning with a <strong>dual-rate optimization</strong>:</p> <ul> <li> <strong>Fast learning</strong>: Quick adaptation to specific examples (like episodic memory)</li> <li> <strong>Slow learning</strong>: Gradual accumulation of general knowledge (like procedural memory)</li> </ul> <div class="algorithm-box"> <strong>Algorithm: Fast-Slow Learning of LTMs</strong> For each training batch: 1. **Fast Learning (Inference-time computation)**: - Initialize variational parameters $(\boldsymbol{\mu}_i, \boldsymbol{\sigma}^2_i)$ for each sequence - For $t = 1$ to $T_{\text{fast}}$ steps: - Sample $\mathbf{z} \sim q_{\boldsymbol{\mu}_i, \boldsymbol{\sigma}^2_i}(\mathbf{z}|\mathbf{x}_i)$ - Compute ELBO: $\mathcal{L}_i = \mathbb{E}_{q}[\log p_{\beta}(\mathbf{x}_i|\mathbf{z})] - \text{KL}(q(\mathbf{z}|\mathbf{x}_i) || p(\mathbf{z}))$ - Update $(\boldsymbol{\mu}_i, \boldsymbol{\sigma}^2_i)$ with high learning rate (0.3) 2. **Slow Learning**: - Update global decoder parameters $\beta$ with low learning rate (0.0004) </div> <p>This reflects the <strong>declarative-procedural framework</strong> from cognitive science—our latent thoughts act like declarative memory while the text generator represents procedural knowledge.</p> <h2 id="key-insights-and-breakthroughs">Key Insights and Breakthroughs</h2> <h3 id="new-scaling-dimensions">New Scaling Dimensions</h3> <p>While traditional language models scale along two main axes (model size and training data), LTMs introduce a third crucial dimension: <strong>inference steps</strong>. More thinking time leads to better performance—you can trade off model size for more deliberate reasoning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blogs/assets/img/ltm/ppl_val_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blogs/assets/img/ltm/ppl_val_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blogs/assets/img/ltm/ppl_val_2-1400.webp"></source> <img src="/blogs/assets/img/ltm/ppl_val_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Scaling behaviors over training tokens and compute.</b> Models with more inference steps demonstrate improved sample efficiency and become compute-efficient beyond certain training compute thresholds. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blogs/assets/img/ltm/scaling_tokens_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blogs/assets/img/ltm/scaling_tokens_new-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blogs/assets/img/ltm/scaling_tokens_new-1400.webp"></source> <img src="/blogs/assets/img/ltm/scaling_tokens_new.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blogs/assets/img/ltm/scaling_flops_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blogs/assets/img/ltm/scaling_flops_new-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blogs/assets/img/ltm/scaling_flops_new-1400.webp"></source> <img src="/blogs/assets/img/ltm/scaling_flops_new.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Scaling behaviors over training tokens and compute.</b> We plot the performance of LTM training runs ($N_{z}=24$) across inference steps ($T_\mathrm{fast}=$16-64) and model sizes (38M-76M). Models with more inference steps demonstrate improved sample efficiency and become compute-efficient beyond certain training compute thresholds. </div> <h3 id="emergent-few-shot-learning-at-small-scale">Emergent Few-Shot Learning at Small Scale</h3> <p>Something remarkable happens with LTMs: they develop few-shot learning abilities (like GPT-3’s in-context learning) but with <strong>dramatically fewer parameters</strong>. Our smallest model achieves this with just 38M parameters—a fraction of what’s typically needed.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blogs/assets/img/ltm/gsm8k-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blogs/assets/img/ltm/gsm8k-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blogs/assets/img/ltm/gsm8k-1400.webp"></source> <img src="/blogs/assets/img/ltm/gsm8k.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Arithmetic reasoning on GSM8K:</b> LTMs with few-shot demonstrations outperform much larger GPT-2 models across various settings. </div> <h3 id="superior-efficiency">Superior Efficiency</h3> <p>The results speak for themselves:</p> <ul> <li> <strong>76% fewer training tokens</strong> needed compared to similar-sized models</li> <li> <strong>93% fewer parameters</strong> than GPT-2-Large while matching its performance</li> <li> <strong>5× faster generation</strong> compared to some diffusion-based alternatives</li> </ul> <h2 id="technical-deep-dive">Technical Deep Dive</h2> <h3 id="model-formulation">Model Formulation</h3> <p>We formulate LTMs within the classical variational Bayes framework. The model assumes latent thought vectors $\mathbf{z}$ follow a prior $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ and generate text $\mathbf{x}$ via a Transformer decoder.</p> <table> <tbody> <tr> <td>We introduce a sequence-specific variational posterior $q(\mathbf{z}</td> <td>\mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma}^2)$ and maximize the evidence lower bound (ELBO):</td> </tr> </tbody> </table> <div class="equation-highlight"> $$\mathcal{L}(\beta, \boldsymbol{\mu}, \boldsymbol{\sigma}^2) = \mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\log p_{\beta}(\mathbf{x}|\mathbf{z})] - \text{KL}(q(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))$$ </div> <p>Crucially, $(\boldsymbol{\mu}, \boldsymbol{\sigma}^2)$ are <strong>local parameters</strong> specific to each sequence, while $\beta$ represents <strong>global parameters</strong> shared across all samples.</p> <h3 id="inference-time-computation">Inference-Time Computation</h3> <p>LTMs introduce a distinct computational cost: <strong>inference-time computation</strong> stemming from the fast learning of latent thought vectors. This occurs in both training and testing.</p> <p>For a model with $L$ layers, $N_{\mathbf{z}}$ latent vectors per layer, and $T_{\text{fast}}$ inference steps, the computational complexity scales as:</p> \[\mathcal{O}(T_{\text{fast}} \cdot L \cdot (N^2H + NN_{\mathbf{z}}H + NH^2))\] <p>When $T_{\text{fast}} \gg 1$, the inference computation dominates, making thinking time the primary computational factor.</p> <h2 id="experimental-results">Experimental Results</h2> <p>We conducted extensive experiments at GPT-2 scale using the OpenWebText dataset. Our results demonstrate:</p> <p><strong>Zero-shot Language Modeling Performance:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Parameters</th> <th>Training FLOPs/tok</th> <th>PTB</th> <th>WikiText</th> <th>LM1B</th> </tr> </thead> <tbody> <tr> <td>GPT-2-Large</td> <td>762M</td> <td>5.32G</td> <td>161.33</td> <td>30.09</td> <td>45.61</td> </tr> <tr> <td>LTM-Medium</td> <td>51M</td> <td>5.52G</td> <td>≤32.06</td> <td>≤17.39</td> <td>≤25.16</td> </tr> <tr> <td>LTM-Large</td> <td>76M</td> <td>32.2G</td> <td>≤<strong>4.43</strong> </td> <td>≤<strong>3.66</strong> </td> <td>≤<strong>3.92</strong> </td> </tr> </tbody> </table> <p><strong>Text Generation Quality:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Sampling</th> <th>MAUVE ↑</th> </tr> </thead> <tbody> <tr> <td>GPT-2-Medium</td> <td>Nucleus-0.95</td> <td>0.955</td> </tr> <tr> <td>GPT-2-Medium</td> <td>Multinomial</td> <td>0.802</td> </tr> <tr> <td>LTM-Large</td> <td>Multinomial</td> <td><strong>0.974</strong></td> </tr> <tr> <td>LTM-Large</td> <td>Greedy</td> <td>0.972</td> </tr> </tbody> </table> <h3 id="probing-the-latent-thoughts">Probing the Latent Thoughts</h3> <p>We investigated how semantic information is distributed across layers through progressive reconstruction experiments. The results reveal that LTMs process information hierarchically:</p> <ul> <li> <strong>6-layer models</strong>: Gradual improvement (~55% accuracy) followed by sharp synthesis in the final layer</li> <li> <strong>12-layer models</strong>: Distributed processing with steady increases through layers 1-8 (~65%) and crucial integration at layers 9-10 (&gt;95% accuracy)</li> </ul> <p>This demonstrates distinctive “synthesis layers” that integrate information from earlier representations.</p> <h2 id="what-this-means-for-ai">What This Means for AI</h2> <h3 id="the-language-of-thought-hypothesis">The Language of Thought Hypothesis</h3> <p>Our approach connects to a deep idea in cognitive science: that thinking happens in an internal “language of thought” that’s distinct from the language we speak. The latent thought vectors can be seen as “words” in this internal cognitive language.</p> <h3 id="inference-time-computation-as-a-new-paradigm">Inference-Time Computation as a New Paradigm</h3> <p>Perhaps most importantly, LTMs demonstrate that <strong>thinking time</strong> can be as valuable as model size or training data. This opens up new possibilities:</p> <ul> <li> <strong>Adaptive computation</strong>: Thinking harder on difficult problems</li> <li> <strong>Resource allocation</strong>: Trading model size for inference time based on computational budgets</li> <li> <strong>Reasoning capabilities</strong>: Using iterative refinement for complex problem-solving</li> </ul> <h2 id="looking-forward">Looking Forward</h2> <p>This work opens several exciting directions:</p> <ol> <li> <strong>Structured Prior Models</strong>: Moving beyond simple Gaussian priors to more sophisticated reasoning structures</li> <li> <strong>Reward-Guided Thinking</strong>: Using reward models to guide the thinking process toward better outcomes</li> <li> <strong>Hierarchical Abstraction</strong>: Developing even more sophisticated multi-level thought representations</li> </ol> <h3 id="current-limitations">Current Limitations</h3> <p>We acknowledge important areas for future work:</p> <ul> <li> <strong>Learnable Prior Models</strong>: Our current Gaussian prior is simple—more structured priors could enable even richer reasoning</li> <li> <table> <tbody> <tr> <td> <strong>Reward Models in Latent Space</strong>: Incorporating verifier models $p_\gamma(r</td> <td>\mathbf{z})$ to guide optimization for reasoning tasks</td> </tr> </tbody> </table> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Latent Thought Models represent a fundamental shift in how we think about language generation. Instead of immediate word-by-word generation, they introduce a more human-like process of abstract thinking followed by linguistic expression.</p> <p>The key insight is simple but profound: <strong>giving AI systems explicit space to think makes them more efficient, more capable, and more aligned with how humans actually process language</strong>.</p> <p>As we continue to push the boundaries of AI capabilities, approaches like LTMs suggest that the future lies not just in bigger models or more data, but in architectures that more closely mirror the sophisticated cognitive processes that make human intelligence so remarkable.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/blogs/assets/img/2025-04-28-distill-example/iclr-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/blogs/assets/img/2025-04-28-distill-example/iclr-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/blogs/assets/img/2025-04-28-distill-example/iclr-1400.webp"></source> <img src="/blogs/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/blogs/assets/bibliography/ltm.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>